{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAtApKyPZnzL"
   },
   "source": [
    "# INF283 | Weekly Exercise 04 | Support Vector Machines and Ensemble Learning\n",
    "\n",
    "#### Deadline\n",
    "October 05, 2018 | 23:59\n",
    "\n",
    "#### What to deliver \n",
    "You can try out your Python code within this notebook. But you should <font color=\"#b51555\">make a PDF file of answers for each of the tasks, and then submit this PDF file </font>on Mitt UiB.  <font color=\"#b51555\">**Do not submit the complete notebook; it makes it difficult for the graders to locate your answers in the lengthy Jupyter notebook. **</font>\n",
    "\n",
    "#### Where to deliver\n",
    "On [Mitt UiB](https://mitt.uib.no/courses/12791/assignments) in the assignments section.\n",
    "\n",
    "#### Note to students\n",
    "This is a Python notebook and all the examples are in Python. You are free to use any programming language you want to do the exercise questions in; we will grade your submissions. But, if you got stuck somewhere, we can provide help for Python only. Moreover, the solution provided after the deadline will also in be in Python.\n",
    "\n",
    "In case you want to learn Python, you can quickly learn it using these [set of short videos](https://www.youtube.com/watch?v=oVp1vrfL_w4&list=PLQVvvaa0QuDe8XSftW-RAxdo6OmaeL85M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9em9UQRYs26"
   },
   "source": [
    "# 1. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HuInMtNKYs26"
   },
   "source": [
    "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tq9_ISySCJye"
   },
   "outputs": [],
   "source": [
    "# one time installation  only\n",
    "!pip install ipywidgets\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIqKbraPYs27"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Use seaborn plotting defaults\n",
    "# Without these defaults, the plots would be a bit uglier\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Some code (such as the code to do PCA) will generate useless warnings\n",
    "# We will suppress these warning using the code below\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQkIQ-QSYs3A"
   },
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vyvlQRyYs3B"
   },
   "source": [
    "Consider the simple case of a classification task, in which the two classes of points are well separated. Lets generate data for such two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60c_3obXYs3C"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# Create 50 (n_samples=50) datapoints clustered together as two (center=2) blobs\n",
    "# The within-cluster spread (cluster_std=0.60) of the datapoints is 0.6\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "# Each datapoint in X is tuple of values (x0, x1). Lets plot them \n",
    "# on x and y axes respectively.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n",
    "plt.xlabel(r\"$x_0$\")\n",
    "plt.ylabel(r\"$x_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pifOqV7y8RkV"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "## <font color=\"#b51555\">Exercise 1.1\n",
    "\n",
    "1. Write the code to find the dimensions of vector `X`\n",
    "2. Write the code to find the dimension of the vector `y` \n",
    "3. What does the vector `y` contain, and what do these values of `y` represent?\n",
    "<font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2q49CkEuYs3K"
   },
   "source": [
    "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
    "For two dimensional data like that shown here, this is a task we could do by hand.\n",
    "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWXmx0ArYs3M"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYiOjwseAjgV"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.2\n",
    "What are `m` and `b` in the `for` loop above? What do they represent?\n",
    "<font>\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pqT9dlgYs3Q"
   },
   "source": [
    "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
    "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1yHJds_Ys3S"
   },
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines offer one way to improve on this.\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whTuZVQgYs3S"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d, col in [(1, 0.65, 0.33, 'r'), (0.5, 1.6, 0.55, 'g'), (-0.2, 2.9, 0.2, 'b')]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k', color=col)\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, \n",
    "                     edgecolor='none',\n",
    "                     color=col, alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exAnOWdzYs3X"
   },
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
    "Support vector machines are an example of such a *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCoeTDlrA7eH"
   },
   "source": [
    "\n",
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.3\n",
    "\n",
    "\n",
    "What is `d` in the code above. What does it represent?\n",
    "<font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWZ_H-sIYs3Z"
   },
   "source": [
    "### Fitting a support vector machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
    "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YmOhjVhYs3Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPExlCYfYs3e"
   },
   "source": [
    "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries and highlight the support vectors  for us :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W96XTx1MYs3f"
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot a black dot over the support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s = 20, facecolor= 'black');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ftq03oxsYs3i"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model, plot_support=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui2AuUC-Ys3m"
   },
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kW67ihiY7JzY"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.4\n",
    "\n",
    "1. Write the code to print out the value of support vectors.\n",
    "2. How many support vectors are there in the model above?\n",
    "<font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmteBAVQYs3r"
   },
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzeo3Kp0Ys3u"
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27jP7k5OYs3x"
   },
   "source": [
    "In the left panel, we see the model and the support vectors for 60 training points.\n",
    "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJDE1y80Ys3y"
   },
   "source": [
    "If you are running this notebook live, you can use IPython's interactive widgets to view this feature of the SVM model interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nyh6HEGwYs30"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[10, 20, 30, 50, 100, 150, 200], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MriWv_m2Ys35"
   },
   "source": [
    "### Beyond linear boundaries: Kernel SVM\n",
    "\n",
    "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
    "We have seen a version of kernels before, in the basis function regressions of  Linear Regression. There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.\n",
    "\n",
    "In SVM models, we can use a version of the same idea.\n",
    "To motivate the need for kernels, let's look at some data that is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHql3b9aYs35"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6kNsnE0Ys4A"
   },
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
    "But we can draw a lesson from the basis function regressions in Linear Regression and think about how we might project the data into a higher dimension such that a linear separator *would* be sufficient.\n",
    "For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKvHDjc7Ys4B"
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-0b1exSYs4I"
   },
   "source": [
    "We can visualize this extra data dimension using a three-dimensional plot—if you are running this notebook live, you will be able to use the sliders to rotate the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnwQOZWsYs4L"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x_0')\n",
    "    ax.set_ylabel('x_1')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=(-90, 90, 5), azim=(-180, 180, 5),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdNtUKNkLaJc"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.5\n",
    "In the section above, we transformed the data from ${\\rm I\\!R^2}$ to ${\\rm I\\!R^3}$ using a third dimension `r`. `r` was obtained by transforming X using a radial basis function. Radial basis function must be centered at some point when calculating them (as you may observed in the Linear Regression assignment). So at which point did we center the `r` while calculating it?\n",
    "<font>\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LotUWVytYs4Q"
   },
   "source": [
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.\n",
    "\n",
    "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYlw5eVGLTRG"
   },
   "source": [
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6q6DpJRYs4R"
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6, probability=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4reEMptYs4U"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YdraYp0Ys4X"
   },
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-35FPNVwrNa"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.6\n",
    "1. Suppose a new point `[0.5, 0]` comes along. Will it be classified as being belong to the `yellow` class or the `red` class. Write the code to classify this point using the model we just trained.\n",
    "2. What is the probability of this point `[0.5, 0]` belonging to the `yellow` class.\n",
    " <font>\n",
    "  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9UTPaKq6zFK"
   },
   "source": [
    "##### NOTE:\n",
    "An SVM classifier, the way it works, cannot give probabilistic information about the class label that it predict. However, there is a work around for it. This method computes the probability that a given datapoint belongs to a particular class using Platt scaling. If you are interested in how Platt Scaling works, then visit this [link](https://prateekvjoshi.com/2015/12/15/how-to-compute-confidence-measure-for-svm-classifiers/).\n",
    "\n",
    "We specified in the code  (using `probability=True` parameter) that we want the SVM to calculate the class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42OUEL7TYs4Z"
   },
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap?\n",
    "For example, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCoCghQOYs4a"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=34, cluster_std=1.1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1j2_Q1SYs4c"
   },
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
    "For very large $C$, the margin is hard, and points cannot lie in it.\n",
    "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPgNbJwaYs4d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqWdFG7NQHeo"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.7\n",
    "How can we know what is the best value of C? What can you do find the best value of C?\n",
    " <font>\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0zud-aHNJZA"
   },
   "source": [
    "## Kernel Trick\n",
    "The Trick in the Kernel Trick is to avoid mapping features from low dimensions to high dimensions, thus avoiding computationally intensive operations.\n",
    "\n",
    "We will now try to elaborate this point with an example. \n",
    "\n",
    "Let us first generate a large dummy dataset `x`, and a data point `z`:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m722h6Lkq_0"
   },
   "outputs": [],
   "source": [
    "x, _ = make_blobs(n_samples=100000, centers=2, random_state=0, cluster_std=1.2)\n",
    "z = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgycbYHmtWps"
   },
   "source": [
    "\n",
    "Now, we will write two functions:\n",
    "\n",
    "**1. `calculate_dot_product_in_higher_dimensional_space`**\n",
    "\n",
    "This function first transforms `x` and `z` in the higher dimensional space using the following transform:\n",
    "$$\n",
    "\\phi(x) = (x_1^2, x_2^2, \\sqrt2 x_1 x_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi(z) = (z_1^2, z_2^2, \\sqrt2 z_1 z_2)\n",
    "$$\n",
    "\n",
    "and then calculates the dot product of $\\phi(x)$ and $\\phi(z)$\n",
    "\n",
    "**2. `calculate_dot_product_in_lower_dimensional_space_with_kernel_trick`**\n",
    "\n",
    "This second function will do the same job as the first one, but it will do it much more efficient without\n",
    "having to the transform the data into the higher dimensional space first. It uses the kernel trick to get the same results as the first function but in a fraction of time of the first function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mpZxZMOu1Xc"
   },
   "outputs": [],
   "source": [
    "def calculate_dot_product_in_higher_dimensional_space(x, z):\n",
    "  \"\"\"\n",
    "  This function calculates the dot product of x and z in higher \n",
    "  dimensional space. This function is highly inefficient.\n",
    "  \"\"\"\n",
    "  \n",
    "  #----------------------------------------------------------------------------#\n",
    "  # Step 1: Transform the data explicitly in higher dimensional space\n",
    "  #----------------------------------------------------------------------------#\n",
    "  \n",
    "  # transform x into higher dimensional space\n",
    "  transformed_x = np.ndarray(shape=(x.shape[0], x.shape[1]+1))\n",
    "  for i in range(x.shape[0]):\n",
    "    transformed_x[i] = np.array([x[i][0]*x[i][0], \n",
    "                                 x[i][1]*x[i][1], \n",
    "                                 sqrt(2)*x[i][0]*x[i][1]])\n",
    "  # transform z into higher dimensional space\n",
    "  transformed_z = np.array([[z[0]*z[0]], \n",
    "                            [z[1]*z[1]], \n",
    "                            [sqrt(2)*z[0]*z[1]]])\n",
    "  \n",
    "  \n",
    "  #----------------------------------------------------------------------------#\n",
    "  # Step 2: Take dot product between each row of transformed_x & transformed_z\n",
    "  #----------------------------------------------------------------------------#\n",
    "\n",
    "  dot_product = np.ndarray(shape=(transformed_x.shape[0], 1))\n",
    "  for i in range(transformed_x.shape[0]):\n",
    "    dot_product[i] = transformed_x[i].T.dot(transformed_z)  \n",
    "  \n",
    "  return dot_product\n",
    "\n",
    "\n",
    "def calculate_dot_product_in_lower_dimensional_space_with_kernel_trick(x, z):\n",
    "  \"\"\" \n",
    "  This function calculates the dot product of x and z in lower dimensional \n",
    "  space (without projecting the data in higher dimensional space first)\n",
    "  using the Kernel trick.\n",
    "  \"\"\"\n",
    "  \n",
    "  #----------------------------------------------------------------------------#\n",
    "  # Step 1: Take dot product of x.T and z in lower dimensional space\n",
    "  #----------------------------------------------------------------------------#\n",
    "  dot_product = x.dot(z)\n",
    "\n",
    "  #----------------------------------------------------------------------------#\n",
    "  # Step 2: Square the dot product and return\n",
    "  #----------------------------------------------------------------------------#\n",
    "  return dot_product**2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQbnk7OHgq-k"
   },
   "source": [
    "Let us now run both the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1AWfLoFgo4N"
   },
   "outputs": [],
   "source": [
    "a = calculate_dot_product_in_higher_dimensional_space(x, z)\n",
    "b = calculate_dot_product_in_lower_dimensional_space_with_kernel_trick(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXe22t8_EzeM"
   },
   "source": [
    "Now we will compare the output of the two function to check if they are the same. The simplest thing we could is to visually check the first five elements of both a and b to see if they are the same. If they are the same, then it means the rest of the elements would also be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBK8EiOtg_1r"
   },
   "outputs": [],
   "source": [
    "print('a:', a[0:5, ])\n",
    "print('b:', b[0:5, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9nNnmLzhFtJ"
   },
   "source": [
    "Comparing the elements of a and b, you can observe that the output of both functions is indeed the same. What is now left is to assess the performance of the two function to check which is faster than the other. We leave it for you as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEPdTsWh_fpq"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.8\n",
    "1. Write the code to compute the run time of both functions?\n",
    "  \n",
    "2. Is the function that implements the kernel trick faster than the other one that does not?\n",
    "<font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yqY6cdnXULG"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.9\n",
    "In the previous exercise, we used the transformation:\n",
    "$$\n",
    "\\phi(x) = (x_1^2, x_2^2, \\sqrt2 x_1 x_2)\n",
    "$$\n",
    " and \n",
    "$$\n",
    "\\phi(z) = (z_1^2, z_2^2, \\sqrt2 z_1 z_2)\n",
    "$$\n",
    " to transform 2-D x and z into 3-D space.\n",
    "  \n",
    "We computed the dot product of $\\phi(x)$ and $\\phi(z)$ in the higher dimensional space. Then, we used a kernel of the form:\n",
    " $$\n",
    "k(x, z) = (x^T z)^2\n",
    "$$\n",
    "that gave us the same results but all the time working in the lower dimension space of `x` and `z`. \n",
    "  \n",
    "  \n",
    "Lets us now change the transformation to something a bit more complicated:\n",
    "$$\n",
    "\\phi(x) = (1, \\sqrt2 x_1, \\sqrt2 x_2, x_1^2, x_2^2, \\sqrt2 x_1 x_2)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\phi(z) = (1, \\sqrt2 z_1, \\sqrt2 z_2, z_1^2, z_2^2, \\sqrt2 z_1 z_2)\n",
    "$$\n",
    "In other words, you are now transforming x and z from 2-D space to 6-D space.\n",
    " \n",
    "Your task is to make appropriate changes in the two functions (1. calculate_dot_product_in_higher_dimensional_space, and 2. calculate_dot_product_in_lower_dimensional_space_with_kernel_trick) that you already have  to handle this new transformation.\n",
    "  \n",
    "\n",
    "If you don't want to work with the pre-written functions, please feel free to write your functions from scratch. Who knows, may be you will implement the function in a much more elegant manner than us.\n",
    "  \n",
    "*HINT*: The dot product of the $\\phi(x)$ and \\$phi(z)$ and the kernel that you should use is given below:\n",
    "\n",
    "  \\begin{array} \n",
    "  {lcl}   \\phi(x)\\phi(z) &=& 1+2x_1z_1+2x_2z_2+x_1^2 z_1^2+x_2^2z_2^2+2x_1z_1x_2z_2 \\\\ & = & (1+x^Tz)^2 \\\\   &=& K(x,z)\\end{array}\n",
    "\n",
    "<font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTQ5G_SsYs4h"
   },
   "source": [
    "## SVM Example: Face Recognition\n",
    "\n",
    "As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
    "We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.\n",
    "A fetcher for the dataset is built into Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UFz1fQpYs4i"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print('faces.target_names:', faces.target_names)\n",
    "print('faces.images.shape:', faces.images.shape)\n",
    "print('number of images', faces.images.shape[0])\n",
    "print('image height:', faces.images.shape[1])\n",
    "print('image width:', faces.images.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HM-RpnkXYs4l"
   },
   "source": [
    "Let's plot a few of these faces to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9ni4XLeYs4n"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5) # 3 rows and 5 columns of images\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_5VQay9Ys4p"
   },
   "source": [
    "Each image contains [62×47] or nearly 3,000 pixels.\n",
    "We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis to extract 150 fundamental components to feed into our support vector machine classifier. You will learn more about principal component analysis in the upcoming lectures. \n",
    "\n",
    "We can do PCA most straightforwardly by packaging the preprocessor and the classifier into a single sklearn pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWeufxCJYs4r"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2O2NwI-fYs4t"
   },
   "source": [
    "For the sake of testing our classifier output, we will split the data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEk-7KMDYs4v"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlBgHJRpYs4y"
   },
   "source": [
    "Finally, we can use a grid search cross-validation to explore combinations of parameters.\n",
    "Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` (which controls the size of the radial basis function kernel), and determine the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPceuj3VYs4z"
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkEBswHbYs43"
   },
   "source": [
    "The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n",
    "\n",
    "Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51-ExT_3Ys44"
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZU9kYzR5Ys47"
   },
   "source": [
    "Let's take a look at a few of the test images along with their predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQELjI5wYs48"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnN6E3QYYs5A"
   },
   "source": [
    "Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s\n",
    "face in the bottom row was mislabeled as Blair).\n",
    "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QulDD-aMYs5A"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DSwYqQoYs5D"
   },
   "source": [
    "We might also display the confusion matrix between these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7IP50UqYs5E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyFBIxQ3Ys5H"
   },
   "source": [
    "This helps us get a sense of which labels are likely to be confused by the estimator.\n",
    "\n",
    "For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation.\n",
    "For this kind of application, one good option is to make use of [OpenCV](http://opencv.org), which, among other things, includes pre-trained implementations of state-of-the-art feature extraction tools for images in general and faces in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jg312ZTaX06B"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 1.10\n",
    "Write a function that takes `ytest` and `yfit` as inputs and then calculates the precision metric by using the number of true positive and false positives.\n",
    "  \n",
    "$$precision = \\frac{TP}{(FP+ TP)}$$\n",
    "  \n",
    "The precision for all the eight classes should be the same as the one obtained using the `classification_report` function.\n",
    "<font>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7diGuzEYs5I"
   },
   "source": [
    "# 2. Ensemble Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDaOy9PHLY-6"
   },
   "source": [
    "Suppose you ask a complex question to thousands of random people, then aggregate their answers. In\n",
    "many cases you will find that this aggregated answer is better than an expert’s answer. This is called the\n",
    "[wisdom of the crowd](https://www.youtube.com/watch?v=iOucwX7Z1HU). Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyebYiPoiXkY"
   },
   "source": [
    "### Voting Classifiers\n",
    "Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a\n",
    "Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors\n",
    "classifier\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and\n",
    "predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier. \n",
    "This voting classifier often achieves a higher accuracy than the best classifier in\n",
    "the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than\n",
    "random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there\n",
    "are a sufficient number of weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcOiDzKx1Tka"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3yh468-1VGk"
   },
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42, n_estimators=100, max_features=1)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), \n",
    "                                          ('svc', svm_clf)],voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CH0IsQPp1jTk"
   },
   "outputs": [],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MzVF6buzPyo"
   },
   "source": [
    "There you have it! The voting classifier slightly outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGYA0VuU2rjs"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 2.1\n",
    "Change the 'voting' parameter from 'hard' to 'soft'. What happens to the accuracy of the voting classifier and why?\n",
    "<font>\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LAuSMTv3_Yt"
   },
   "source": [
    "### Bagging Ensembles\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed.\n",
    "Another approach is to use the same training algorithm for every predictor, but to train them on different\n",
    "random subsets of the training set. When sampling is performed with replacement, this method is called\n",
    "bagging (short for bootstrap aggregating). Bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply\n",
    "aggregating the predictions of all predictors. The aggregation function is typically the statistical mode\n",
    "(i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for\n",
    "regression. Each individual predictor has a higher bias than if it were trained on the original training set,\n",
    "but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a\n",
    "similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "Let us train a bagging classifier of 500 decision trees and compare its performance to that of single tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RRefnud4Fze"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=42), \n",
    "                            n_estimators=500,\n",
    "                            max_samples=100, \n",
    "                            bootstrap=True, \n",
    "                            n_jobs=-1, \n",
    "                            random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3yDv_-l5bEs"
   },
   "source": [
    "Let us check the performance of this bagging classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoVa3jtW4PLm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVentDDi5UDy"
   },
   "source": [
    "Let us now see the performance of a simple decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1O3US5l4Us4"
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_E7lAlBg5j0z"
   },
   "source": [
    "As you can see, the bagging classifier (based on 500 decision trees) performs much better than a single descision tree alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHWmCYnhju2D"
   },
   "source": [
    "---\n",
    "## <font color=\"#b51555\">Exercise 2.2\n",
    "Use the abalone dataset from UCI\n",
    "  https://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "and use apply AdaBoostClassifier, BaggingClassifier, RandomForest, and DecisionTreeClassifier and see which one gets the best performance. \n",
    " \n",
    "*HINTS:*\n",
    "  \n",
    "  You don't need to download the data as a file on your computer and then read it in Python. You can read in Python it directly from the data URL using the following code snippet:\n",
    "  ```\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"\n",
    "s = requests.get(url).content\n",
    "  \n",
    "# save the data in a dataframe\n",
    "df = pd.read_csv(io.StringIO(s.decode('utf-8')), header=None)\n",
    "  ```\n",
    "Now you have a pandas dataframe `df`, split it into `X` and `y`. `y` should contain the gender of the abalone.\n",
    "  In the df, the sex of abalone is in the first column, and has three possible values: M (male), F (female), and I (Infant). Encode these as numbers (0, 1, and 2) using LabelEncoder in sklearn. Then split X, and y into train and test and use them for training and testing your classifiers.\n",
    "  \n",
    "<font>\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmGtFdavYs5I"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Sources and Attributions:\n",
    "1. Python Data Science Handbook by Jake VanderPlas\n",
    "2. Implementing a Weighted Majority Rule Ensemble Classifier in scikit-learn by Sebastian Raschka\n",
    "3. \tHands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems by Geron Aurelien\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INF-283 Weekly Exercise 04.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
