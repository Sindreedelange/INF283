{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible results\n",
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(one_hot_enc=True, reshape=False, train_val_test=False):\n",
    "    DATA_PATH = \"../data/\"\n",
    "    file_list = os.listdir(DATA_PATH)\n",
    "    images_path = file_list[0]\n",
    "    labels_path = file_list[1]\n",
    "    images_path_full = os.path.join(DATA_PATH + images_path)\n",
    "    labels_path_full = os.path.join(DATA_PATH + labels_path)\n",
    "    X = pd.read_csv(images_path_full)\n",
    "    if reshape:\n",
    "        X = X.values.astype('float32')\n",
    "        # Normalize data\n",
    "        X = X / 255\n",
    "        # Reshape for cnn \n",
    "        X = X.reshape([-1, 28, 28, 1]).astype('float32')\n",
    "    else:\n",
    "        # Normalize data\n",
    "        X = X / 255\n",
    "    y = pd.read_csv(labels_path_full)\n",
    "    # Encode labels\n",
    "    if one_hot_enc:\n",
    "        encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "        y = encoder.fit_transform(y)\n",
    "    # Divide into train and test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    if train_val_test:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=random_state)\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    else:\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_shapes_val(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    print(\"X_train_nn: \", X_train.shape)\n",
    "    print(\"X_val_nn: \", X_val.shape)\n",
    "    print(\"X_test_nn: \", X_test.shape)\n",
    "    print(\"y_train_nn: \", y_train.shape)\n",
    "    print(\"y_val_nn: \", y_val.shape)\n",
    "    print(\"y_test_nn: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_shapes_nval(X_train, X_test, y_train, y_test):\n",
    "    print(\"X_train_nn: \", X_train.shape)\n",
    "    print(\"X_test_nn: \", X_test.shape)\n",
    "    print(\"y_train_nn: \", y_train.shape)\n",
    "    print(\"y_test_nn: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delsin\\AppData\\Local\\Continuum\\anaconda3\\envs\\dat158\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn, X_val_nn, X_test_nn, y_train_nn, y_val_nn, y_test_nn = get_data(train_val_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "# Model compile\n",
    "adam = Adam(lr=1e-3)\n",
    "metrics = ['accuracy']\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "# Model architecture\n",
    "regularizer_l2 = regularizers.l2(0.01)\n",
    "\n",
    "# Model fit\n",
    "epochs = 12\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nn:  (39199, 784)\n",
      "X_val_nn:  (16800, 784)\n",
      "X_test_nn:  (14000, 784)\n",
      "y_train_nn:  (39199, 10)\n",
      "y_val_nn:  (16800, 10)\n",
      "y_test_nn:  (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "print_data_shapes_val(X_train_nn, X_val_nn, X_test_nn, y_train_nn, y_val_nn, y_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784, activation='relu', kernel_regularizer=regularizer_l2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu', kernel_regularizer=regularizer_l2))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=adam, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39199 samples, validate on 16800 samples\n",
      "Epoch 1/12\n",
      "39199/39199 [==============================] - 3s 77us/step - loss: 1.0281 - acc: 0.8396 - val_loss: 0.5649 - val_acc: 0.9104\n",
      "Epoch 2/12\n",
      "39199/39199 [==============================] - 3s 69us/step - loss: 0.4634 - acc: 0.9327 - val_loss: 0.4888 - val_acc: 0.9145\n",
      "Epoch 3/12\n",
      "39199/39199 [==============================] - 2s 63us/step - loss: 0.3810 - acc: 0.9469 - val_loss: 0.3731 - val_acc: 0.9454\n",
      "Epoch 4/12\n",
      "39199/39199 [==============================] - 3s 65us/step - loss: 0.3401 - acc: 0.9502 - val_loss: 0.3545 - val_acc: 0.9470\n",
      "Epoch 5/12\n",
      "39199/39199 [==============================] - 3s 64us/step - loss: 0.3158 - acc: 0.9533 - val_loss: 0.3380 - val_acc: 0.9470\n",
      "Epoch 6/12\n",
      "39199/39199 [==============================] - 3s 65us/step - loss: 0.2808 - acc: 0.9580 - val_loss: 0.3180 - val_acc: 0.9463\n",
      "Epoch 7/12\n",
      "39199/39199 [==============================] - 3s 66us/step - loss: 0.2703 - acc: 0.9594 - val_loss: 0.2996 - val_acc: 0.9479\n",
      "Epoch 8/12\n",
      "39199/39199 [==============================] - 3s 68us/step - loss: 0.2554 - acc: 0.9614 - val_loss: 0.2708 - val_acc: 0.9574\n",
      "Epoch 9/12\n",
      "39199/39199 [==============================] - 3s 66us/step - loss: 0.2447 - acc: 0.9610 - val_loss: 0.3024 - val_acc: 0.9464\n",
      "Epoch 10/12\n",
      "39199/39199 [==============================] - 3s 65us/step - loss: 0.2394 - acc: 0.9632 - val_loss: 0.2555 - val_acc: 0.9569\n",
      "Epoch 11/12\n",
      "39199/39199 [==============================] - 3s 66us/step - loss: 0.2257 - acc: 0.9651 - val_loss: 0.2645 - val_acc: 0.9534\n",
      "Epoch 12/12\n",
      "39199/39199 [==============================] - 2s 64us/step - loss: 0.2276 - acc: 0.9639 - val_loss: 0.2390 - val_acc: 0.9625\n",
      "14000/14000 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "start_nn = time.time()\n",
    "model.fit(X_train_nn, y_train_nn, validation_data=(X_val_nn, y_val_nn), epochs=epochs, batch_size=batch_size)\n",
    "end_nn = time.time()\n",
    "scores = model.evaluate(X_test_nn, y_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.24156733674662453, 0.9622142857142857] \n",
      " Training time: 1540287525.74  seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores: \", scores, \"\\n Training time: %.2f\" % end_nn, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can see that the model is somewhat overfitting - Will try to implement *Regularizer*\n",
    "<br>\n",
    "This resulted in a loss in accuracy, but there is a trade off between accuracy and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, X_val_cnn, X_test_cnn, y_train_cnn, y_val_cnn, y_test_cnn = get_data(one_hot_enc=False, reshape=True, train_val_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cnn = np_utils.to_categorical(y_train_cnn)\n",
    "y_val_cnn = np_utils.to_categorical(y_val_cnn)\n",
    "y_test_cnn = np_utils.to_categorical(y_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nn:  (39199, 28, 28, 1)\n",
      "X_val_nn:  (16800, 28, 28, 1)\n",
      "X_test_nn:  (14000, 28, 28, 1)\n",
      "y_train_nn:  (39199, 10)\n",
      "y_val_nn:  (16800, 10)\n",
      "y_test_nn:  (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "print_data_shapes_val(X_train_cnn, X_val_cnn, X_test_cnn, y_train_cnn, y_val_cnn, y_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39199 samples, validate on 16800 samples\n",
      "Epoch 1/4\n",
      "39199/39199 [==============================] - 16s 407us/step - loss: 0.2035 - acc: 0.9373 - val_loss: 0.0751 - val_acc: 0.9764\n",
      "Epoch 2/4\n",
      "39199/39199 [==============================] - 16s 414us/step - loss: 0.0624 - acc: 0.9803 - val_loss: 0.0598 - val_acc: 0.9818\n",
      "Epoch 3/4\n",
      "39199/39199 [==============================] - 16s 417us/step - loss: 0.0437 - acc: 0.9858 - val_loss: 0.0519 - val_acc: 0.9843\n",
      "Epoch 4/4\n",
      "39199/39199 [==============================] - 16s 415us/step - loss: 0.0342 - acc: 0.9890 - val_loss: 0.0461 - val_acc: 0.9868\n",
      "14000/14000 [==============================] - 2s 143us/step\n"
     ]
    }
   ],
   "source": [
    "start_cnn = time.time()\n",
    "model.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=4, batch_size=200)\n",
    "end_cnn = time.time()\n",
    "scores = model.evaluate(X_test_cnn, y_test_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.04617324691006382, 0.9867142857142858] \n",
      " Training time: 1540287597.83  seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores: \", scores, \"\\n Training time: %.2f\" % end_cnn, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nn:  (55999, 784)\n",
      "X_test_nn:  (14000, 784)\n",
      "y_train_nn:  (55999, 10)\n",
      "y_test_nn:  (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "print_data_shapes_nval(X_train_rf, X_test_rf, y_train_rf, y_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_f_clf = RandomForestClassifier(n_estimators=50,\n",
    "                                     n_jobs = 2,\n",
    "                                     random_state=random_state)\n",
    "start_rf = time.time()\n",
    "random_f_clf.fit(X_train_rf, y_train_rf)\n",
    "end_rf = time.time()\n",
    "y_pred_rf = random_f_clf.predict(X_test_rf)\n",
    "scores = accuracy_score(y_test_rf, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  0.8957857142857143 \n",
      " Training time: 1540287621.94  seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores: \", scores, \"\\n Training time: %.2f\" % end_rf, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needing to install xgboost (using conda)\n",
    "# ! conda install py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xg, X_test_xg, y_train_xg, y_test_xg = get_data(one_hot_enc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_nn:  (55999, 784)\n",
      "X_test_nn:  (14000, 784)\n",
      "y_train_nn:  (55999, 1)\n",
      "y_test_nn:  (14000, 1)\n"
     ]
    }
   ],
   "source": [
    "print_data_shapes_nval(X_train_xg, X_test_xg, y_train_xg, y_test_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'gamma': [0.5, 2],\n",
    "        'subsample': [0.6, 1.0],\n",
    "        'colsample_bytree': [0.6, 1.0],\n",
    "        'max_depth': [3, 5],\n",
    "        'n_estimators': [5, 10]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun to try with more parameters, but my poor CPU cannot take any more.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer: This takes a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "xgb_clf = XGBClassifier()\n",
    "rs = GridSearchCV(xgb_clf,\n",
    "                  params,\n",
    "                  cv=2,\n",
    "                  scoring=\"accuracy\",\n",
    "                  n_jobs=1,\n",
    "                  verbose=2)\n",
    "start_xgb = time.time()\n",
    "rs.fit(X_train_xg, y_train_xg.values.ravel())\n",
    "end_xgb = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_est_xg = rs.best_estimator_\n",
    "print(optim_est_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xg = optim_est_xg.predict(X_test_xg)\n",
    "scores = accuracy_score(y_test_xg, y_pred_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scores: \", scores, \"\\n Training time: %.2f\" % end_xgb, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
